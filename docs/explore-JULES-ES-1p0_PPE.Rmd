---
title: "explore-JULES-ES-1p0_PPE"
author: "Doug McNeall"
date: "2020-10-16"
output: html_document
---

### Load helper functions
```{r, echo = FALSE, message = FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.path = "figs/", echo = FALSE, message = FALSE, warnings = FALSE)
# load some helper functions
source('explore-JULES-ES-1p0_PPE_functions.R')
```

### Load packages
```{r, message = FALSE, warning=FALSE}
  library(RColorBrewer)
  library(fields)
  library(MASS)
  library(DiceKriging)
  library(ncdf4)
  library(ncdf4.helpers)
  source("https://raw.githubusercontent.com/dougmcneall/packages-git/master/emtools.R")
  source("https://raw.githubusercontent.com/dougmcneall/packages-git/master/imptools.R")
  source("https://raw.githubusercontent.com/dougmcneall/packages-git/master/vistools.R")

```

## Preliminaries
```{r}
# Some pallete options
yg = brewer.pal(9, "YlGn")
ryb = brewer.pal(11, "RdYlBu")
byr = rev(ryb)
rb = brewer.pal(11, "RdBu")
br = rev(rb)
blues = brewer.pal(9, 'Blues')
cbPal <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")


```


```{r}
# data location

ensloc <- '/project/carbon_ppe/JULES-ES-1p0_PPE/'
ensmember <- 'P0000/'
subdir <- 'stats/'

floc <- paste0(ensloc,ensmember,subdir)
fn <- 'JULES-ES-1p0_P0000_Annual_global.nc'

```

```{r}
# test file
nc <- nc_open(paste0(floc,fn))

# What variables are in the file? 
varlist <- nc.get.variable.list(nc)

```

```{r}
# for each of the variables in the file, average the last 20 years as the "modern" value,
# and then place in a matrix

modern_value <- function(nc, variable, ix){
  # A basic function to read a variable and 
  # take the mean of the timeseries at locations ix
  dat <- ncvar_get(nc, variable)
  out <- mean(dat[ix])
  out
}
#144:164 is the 1993:2013
modern_value(nc = nc, variable = "npp_nlim_lnd_mean", ix = 144:164)

# apply to the test file to check it works
vec <- sapply(varlist, FUN = modern_value, nc = nc, ix = 144:164)

```


```{r, cache = TRUE, warning = FALSE, message = FALSE, results = FALSE}
# Generate ensemble numbers
nens = 499

datmat <- matrix(nrow = nens, ncol = length(varlist))
colnames(datmat) <- varlist

enslist <- paste("P", formatC(0:(nens-1), width=4, flag="0"), sep="")

floc <- paste0(ensloc,ensmember,subdir)

for(i in 1:nens){
  
  vec <- rep(NA, length(varlist))
  
  ensmember <- enslist[i] 
  
  fn <- paste0(ensloc,ensmember,'/stats/','JULES-ES-1p0_',ensmember,'_Annual_global.nc')
  #print(fn)
  
  try(nc <- nc_open(paste0(fn)))
  try(vec <- sapply(varlist, FUN = modern_value, nc = nc, ix = 144:164))
  datmat[i, ] <- vec
  nc_close(nc)
}
```


```{r}
# 
nlevel0.ix <- which(is.na(datmat[,3]))

# Y is the whole data set
Y <- datmat
# Y0 is the 'cleaned' data set, truncated to variables that change, and removing NAs
Y.level0 <- datmat[-nlevel0.ix, -c(2,30,31)]
Y.nlevel0 <- datmat[nlevel0.ix, -c(2,30,31)]

```



```{r}
years = 1864:2013
ysec = 60*60*24*365
#norm.vec = c(1e12, 1e12, 1e12/ysec , 1e12, 1e12, 1e9)

# Load up the data
lhs_i = read.table('~/brazilCSSP/code/brazil_cssp/analyze_u-ao732/data/ES_PPE_i/lhs_u-ao732.txt', header = TRUE)
lhs_ii = read.table('~/brazilCSSP/code/brazil_cssp/analyze_u-ao732/data/ES_PPE_ii/lhs_u-ao732a.txt', header = TRUE)

toplevel.ix = 1:499

# The raw input data is a latin hypercube
lhs = rbind(lhs_i, lhs_ii)[toplevel.ix, ]
lhs.level0 <- lhs[-nlevel0.ix,]


X = normalize(lhs)
colnames(X) = colnames(lhs)

X.level0 <- X[-nlevel0.ix,]
X.nlevel0 <- X[nlevel0.ix,]

d = ncol(X)
# lower and higher bound on the normalised matrix for visualisation
rx = rbind(rep(0,32), rep(1,32))
```


## Where did the model fail to run?

There are clear run failure thresholds in the parameters rootd_ft_io and lai_max, and quite strong visual indications that a_wl_io and bio_hum_cn matter.

```{r, fig.width = 12, fig.height = 12}

#simple way
pairs(X.nlevel0, xlim = c(0,1), ylim = c(0,1),col = 'red', gap = 0, lower.panel = NULL, pch = 20)

# plot failures over everything else
X.all <- rbind(X.level0, X.nlevel0)
colvec <- rep('grey', nrow(X))
colvec[(nrow(X.level0)+1):nrow(X.all)] <- 'red'

pairs(X.all, xlim = c(0,1), ylim = c(0,1),col = colvec, gap = 0, lower.panel = NULL, pch = 20, cex = 0.5)

```

## Histograms of run failures

```{r, fig.width = 10, fig.height = 10}

p = ncol(X.level0)

par(mfrow = c(6,6), mar = c(3,2,3,2), fg = 'lightgrey')

for(i in 1:p){
  hist(X.nlevel0[,i], main = colnames(X.nlevel0)[i], xlab = '', ylab = '', col = 'lightgrey', xlim = c(0,1),
       breaks = 10)
}



```




# Histograms of the output at Level 0 (places the model ran and produced output) 

```{r, fig.width = 12, fig.height = 12}
d = ncol(Y.level0)

par(mfrow = c(5,6), fg = 'lightgrey', mar = c(3,2,3,2))

for(i in 1:d){
  hist(Y.level0[,i], main = colnames(Y.level0)[i], xlab = '', ylab = '', col = 'lightgrey')
}

```

## Test out some emulators
At the moment, we'll just keep to the things that we know should work.
We'll use mean NPP as an example

Andy would like to see timeseries of:

cVeg, cSoil and nbp, npp in GtC and GtC/yr.

```{r, fig.width = 12, fig.height = 12}
p <- ncol(X.level0)

y.level0 <- Y.level0[,'npp_nlim_lnd_sum']

par(mfrow = c(5,7), mar = c(3,1,3,1))
for(i in 1:p){
  
  plot(X.level0[,i], y.level0, xlab = '', ylab = '', main = colnames(X.level0)[i])
}

```

## A clear threshold in the F0 parameter.
It appears that this ensemble is less "clear cut" in having an output that clearly distinguishes between "failed" (or close to it), and "not failed".
Having said that, having an F0 over a threshold seems to kill the carbon cycle, as before.

```{r}

par(mfrow = c(2,1))
plot(lhs$f0_io, datmat[, 'npp_nlim_lnd_sum'])
plot(X.level0[,'f0_io'], Y.level0[, 'npp_nlim_lnd_sum'])
abline(v = 0.9)

```



# A DiceKriging emulator for npp
```{r, fig.width = 10, fig.height = 10, results = 'hide'}

em <- km(~., design = X.level0, response = y.level0)
plot(em)

```


```{r}

ts.glmnet.em <- twoStep.glmnet(X = X.level0, y = y.level0)
plot(ts.glmnet.em$emulator)

```



# Remove anything with f0 over a threshold.
```{r}
level1.ix <- which(X.level0[, 'f0_io'] < 0.9)

X.level1 <- X.level0[level1.ix, ]
Y.level1 <- Y.level0[level1.ix,]

y.level1 <- Y.level1[, 'npp_nlim_lnd_sum']
em.level1 <- km(~., design = X.level1,  response = y.level1)
```


```{r, fig.width = 10, fig.height = 10}

plot(X.level1[, 'f0_io'], Y.level1[, 'npp_nlim_lnd_sum'])

plot(em.level1)

```


```{r}

ts.glmnet.em.level1 <- twoStep.glmnet(X = X.level1, y = y.level1)
plot(ts.glmnet.em.level1$emulator )

```


```{r}

source('~/brazilCSSP/code/brazil_cssp/per_pft.R' )

    
loo <- twoStep.sens(X = X.level1, y = y.level1)

```

```{r, fig.width = 10, fig.height = 5}
par(las = 2, mar = c(9,5,3,1))
plot(loo, axes = FALSE, xlab = '', ylab = '')
axis(1, at = 1:p, labels = colnames(X) )
axis(2)
```
```{r, cache = TRUE}

yvec <- c('nbp_lnd_sum', 'fLuc_lnd_sum', 'npp_nlim_lnd_sum', 'cSoil_lnd_sum',
          'cVeg_lnd_sum', 'landCoverFrac_lnd_sum', 'fHarvest_lnd_sum',
          'lai_lnd_sum', 'rh_lnd_sum', 'treeFrac_lnd_sum', 'c3PftFrac_lnd_sum', 
          'c4PftFrac_lnd_sum', 'shrubFrac_lnd_sum', 'baresoilFrac_lnd_sum')

oat.var.sensmat <- matrix(NA, nrow = length(yvec), ncol = ncol(X.level1))

for(i in 1:length(yvec)){
  
  yname <- yvec[i]
  y <- Y.level1[, yname]
  loo <- twoStep.sens(X = X.level1, y = y)
  oat.var.sensmat[i, ] <- loo 
}
```


## How good are the emulators for each output?
The emulators appear to be at least capturing the broad response for all of the output variables, even if there are different

```{r, fig.width = 12, fig.height = 6, cache = TRUE}
emlist.twoStep.glmnet <- vector(mode = 'list', length = length(yvec))
for(i in 1:length(yvec)){
  
  yname <- yvec[i]
  y <- Y.level1[, yname]
  
  em <- twoStep.glmnet(X = X.level1, y = y)
  emlist.twoStep.glmnet[[i]] <- em
}
```


```{r}
loolist.twoStep.glmnet <- vector(mode = 'list', length = length(yvec))

for(i in 1:length(yvec)){
  
  loo <- leaveOneOut.km(model = emlist.twoStep.glmnet[[i]]$emulator, type = 'UK', trend.reestim = TRUE)
  
  loolist.twoStep.glmnet[[i]] <- loo

}
```


```{r, fig.width = 12, fig.height = 12}
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,0.1,0.1))
for(i in 1:length(loolist.twoStep.glmnet )){
  
  y <- Y.level1[, yvec[i]]
  loo <- loolist.twoStep.glmnet[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = colnames(Y.level1)[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2)    

```

## Compare the previous (twostep algorithm) with straight km output
```{r, fig.width = 12, fig.height = 6, cache = TRUE, results = 'hide'}
emlist.km <- vector(mode = 'list', length = length(yvec))
#for(i in 1:length(yvec)){
for(i in 1:length(yvec)){
  yname <- yvec[i]
  y <- Y.level1[, yname]
    
  em <- km(~., design = X.level1, response = y)
  emlist.km[[i]] <- em
}
```



```{r}
loolist.km <- vector(mode = 'list', length = length(yvec))

for(i in 1:length(yvec)){
  
  loo <- leaveOneOut.km(model = emlist.km[[i]], type = 'UK', trend.reestim = TRUE)
  
  loolist.km[[i]] <- loo

}
```

```{r, fig.width = 12, fig.height = 12}
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,0.1,0.1))
for(i in 1:length(loolist.km)){
  
  y <- Y.level1[, yvec[i]]
  loo <- loolist.km[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = colnames(Y.level1)[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 

```



# Heatmaps of sensitivity analysis across a number of variables

```{r, fig.width= 10, fig.height = 6}
# normalize the sensitivity matrix
colnames(oat.var.sensmat) <- colnames(X.level1)
rownames(oat.var.sensmat) <- yvec

#test <- normalize(t(oat.var.sensmat))

#image(test)
#par()
heatmap(oat.var.sensmat, Rowv = NA, Colv = NA, mar = c(10,10), scale = 'row')
heatmap(oat.var.sensmat, mar = c(10,10), scale = 'row')

```
```{r, fig.width =12, fig.height = 7}

normsens <- normalize(t(oat.var.sensmat))

par(mar = c(10,12,5,1))
image(normsens, axes = FALSE, col = blues)
axis(1, at = seq(from = 0, to = 1, length.out = p), labels = colnames(X.level1), las = 2)
axis(2, at = seq(from = 0, to = 1, length.out = length(yvec)), labels = yvec, las = 1)
mtext('One-at-a-time sensitivity, variance across level 1 ensemble', side = 3, adj = 0, line = 2, cex = 1.8)

```


Write a function that analyses the quality of emulators
```{r}

kmqual <- function(kmobj){
  leaveOneOut.km(kmobj)
  
  
}


```


# Constraint
```{r, fig.width = 8, fig.height = 8}
ynames.const <- c('nbp_lnd_sum', 'npp_nlim_lnd_sum', 'cSoil_lnd_sum', 'cVeg_lnd_sum')
yunits.const <- c('GtC/year', 'GtC/year', 'GtC', 'GtC')
Y.const.level1 <- Y.level1[, ynames.const]

scalevec <- c(1e12/ysec, 1e12/ysec, 1e12, 1e12)
Y.const.level1.scaled <- sweep(Y.const.level1,2, STATS = scalevec, FUN = '/' )


level2.ix = which(Y.const.level1.scaled[,'nbp_lnd_sum'] > -10 &
                    Y.const.level1.scaled[,'npp_nlim_lnd_sum'] > 35 &  Y.const.level1.scaled[,'npp_nlim_lnd_sum'] < 80 &
                    Y.const.level1.scaled[,'cSoil_lnd_sum'] > 750 & Y.const.level1.scaled[,'cSoil_lnd_sum'] < 3000 &
                  Y.const.level1.scaled[,'cVeg_lnd_sum'] > 300 & Y.const.level1.scaled[,'cVeg_lnd_sum'] < 800
  )
```





```{r, fig.width = 10, fig.height = 7}

# Histogram of level 1 constraints
hcol = 'darkgrey'
lcol = 'black'
par(mfrow = c(2,2), fg = 'darkgrey', las = 1)



hist(Y.const.level1.scaled[,'nbp_lnd_sum'], col = hcol, main = 'NBP', xlab = 'GtC/year')
polygon(x = c(-10, 100, 100, -10), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
        border = makeTransparent('tomato2'))
hist(Y.const.level1.scaled[,'cSoil_lnd_sum'], col = hcol, main = 'Soil Carbon', xlab = 'GtC')
polygon(x = c(750, 3000, 3000, 750), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
        border = makeTransparent('tomato2'))

hist(Y.const.level1.scaled[,'cVeg_lnd_sum'], col = hcol, main = 'Vegetation Carbon', xlab = 'GtC')
polygon(x = c(300, 800, 800, 300), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
       border =  makeTransparent('tomato2'))
hist(Y.const.level1.scaled[,'npp_nlim_lnd_sum'], col = hcol , main = 'NPP', xlab = 'GtC/year')
polygon(x = c(35, 80, 80, 35), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
        border = makeTransparent('tomato2')
)

```


## Ensemble members that comply with constraints in NPP, NBP, soil and vegetation carbon.

```{r, fig.width = 12, fig.height = 12}

X.const <- X.level1[ level2.ix , ]

pairs(X.const, xlim = c(0,1), gap = 0, pch = 20, lower.panel = NULL)


```

## Build an emulator for each output that we have constraints for, and find the input space where those constraints are met.

```{r, cache = TRUE}

mins <- apply(X.level1,2,FUN = min)
maxes <- apply(X.level1,2,FUN = max)

nsamp.unif = 100000  
X.unif = samp.unif(nsamp.unif, mins = mins, maxes = maxes)

Y.unif = matrix(nrow = nsamp.unif, ncol = ncol(Y.const.level1.scaled))

colnames(Y.unif) = colnames(Y.const.level1.scaled)
  
emlist.const <- vector(mode = 'list', length = ncol(Y.const.level1.scaled))

# Build an emulator for each output individually
for(i in 1:ncol(Y.unif)){
  em <- twoStep.glmnet(X = X.level1, y = Y.const.level1.scaled[,i])
  emlist.const[[i]] <- em
  pred = predict(em$emulator, newdata = X.unif, type = 'UK')
  Y.unif[,i] <- pred$mean
}


# Find the inputs where the mean of the emulated output is 
# within the tolerable limits set by the modeller.

level2.unif.kept.ix = which(Y.unif[,'nbp_lnd_sum'] > -10 &
                    Y.unif[,'npp_nlim_lnd_sum'] > 35 &  Y.unif[,'npp_nlim_lnd_sum'] < 80 &
                    Y.unif[,'cSoil_lnd_sum'] > 750 & Y.unif[,'cSoil_lnd_sum'] < 3000 &
                  Y.unif[,'cVeg_lnd_sum'] > 300 & Y.unif[,'cVeg_lnd_sum'] < 800
  )


X.kept = X.unif[level2.unif.kept.ix , ]

# we've removed 80% of our prior input space
(nrow(X.kept) / nsamp.unif) * 100

ix.rejected = setdiff(1:nsamp.unif, level2.unif.kept.ix)
X.rejected = X.unif[ix.rejected, ]



```



## Pairs plot of input space that is Not Ruled Out Yet when basic constraints are applied to annual data.

```{r emulator_pairs, fig.width = 10, fig.height = 10, warning = FALSE, message = FALSE}

par(oma = c(0,0,0,3), bg = 'white')
pairs(X.kept,
      labels = 1:d,
      gap = 0, lower.panel = NULL, xlim = c(0,1), ylim = c(0,1),
      panel = dfunc.up,
      cex.labels = 1,
      col.axis = 'white',
      dfunc.col = blues)

image.plot(legend.only = TRUE,
           zlim = c(0,1),
           col = blues,
           legend.args = list(text = 'Density of model runs matching the criteria', side = 3, line = 1),
           horizontal = TRUE
)

legend('left', legend = paste(1:d, colnames(lhs)), cex = 0.9, bty = 'n')

```

# Marginal histograms of space that is Not Ruled Out Yet when basic constraints are applied.

```{r, fig.width = 12, fig.height = 12}

par(mfrow = c(8,4), fg = 'grey', mar = c(3,2,2,1))

for(i in 1:ncol(X.unif)){
  
  hist(X.kept[,i], col = 'grey', main = colnames(X.unif)[i], xlim = c(0,1),axes = FALSE, xlab = '', ylab = '')
  axis(1)
  
}



```


```{r}

par(mfrow = c(2,2))
for(i in 1:ncol(Y.const.level1.scaled)){
hist(Y.const.level1.scaled[ ,i], main = colnames(Y.const.level1.scaled)[i], xlab = yunits.const[i])
}

```


```{r}


#test <- twoStep.glmnet(X = X)

#for(i in 1:ncol(Y0)){
  
#    em = twoStep.glmnet(X = X0, y = Y0[,i])
#     global.emlist[[i]] = em
#pred = predict(em$emulator, newdata = X.unif, type = 'UK')
#y.unif[,i] = pred$mean
#}



```




