---
title: "explore-JULES-ES-1p0_PPE"
author: "Doug McNeall"
date: "2020-10-21"
output: html_document
---

# Sensitivity analysis and constraint of the parameter space of Earth System configuration of JULES.
Uses global mean from 1994-2013 of several carbon-cycle outputs to rule out parts of input space. 

```{r, echo = FALSE, message = FALSE, warning=FALSE}
# Load helper functions

knitr::opts_chunk$set(fig.path = "figs/", echo = FALSE, message = FALSE, warnings = FALSE)
# load some helper functions
source('~/brazilCSSP/code/brazil_cssp/per_pft.R') # eventually, move the relevant functions
source('explore-JULES-ES-1p0_PPE_functions.R')
```

```{r, message = FALSE, warning=FALSE}
# Load packages

  library(RColorBrewer)
  library(fields)
  library(MASS)
  library(DiceKriging)
  library(ncdf4)
  library(ncdf4.helpers)
  source("https://raw.githubusercontent.com/dougmcneall/packages-git/master/emtools.R")
  source("https://raw.githubusercontent.com/dougmcneall/packages-git/master/imptools.R")
  source("https://raw.githubusercontent.com/dougmcneall/packages-git/master/vistools.R")
```

```{r}
# Preliminaries
# Some pallete options
yg = brewer.pal(9, "YlGn")
ryb = brewer.pal(11, "RdYlBu")
byr = rev(ryb)
rb = brewer.pal(11, "RdBu")
br = rev(rb)
blues = brewer.pal(9, 'Blues')
cbPal <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


```{r}
# data location

ensloc <- '/project/carbon_ppe/JULES-ES-1p0_PPE/'
ensmember <- 'P0000/'
subdir <- 'stats/'

floc <- paste0(ensloc,ensmember,subdir)
fn <- 'JULES-ES-1p0_P0000_Annual_global.nc'

# test file
nc <- nc_open(paste0(floc,fn))

# What variables are in the file? 
varlist <- nc.get.variable.list(nc)

```

```{r, results = 'hide'}
# for each of the variables in the file, average the last 20 years as the "modern" value,
# and then place in a matrix

modern_value <- function(nc, variable, ix){
  # A basic function to read a variable and 
  # take the mean of the timeseries at locations ix
  dat <- ncvar_get(nc, variable)
  out <- mean(dat[ix])
  out
}
#144:164 is the 1993:2013
modern_value(nc = nc, variable = "npp_nlim_lnd_mean", ix = 144:164)

# apply to the test file to check it works
vec <- sapply(varlist, FUN = modern_value, nc = nc, ix = 144:164)

```


```{r, warning = FALSE, message = FALSE, results = FALSE}
# Generate ensemble numbers, mean of the last 20 years of the timeseries (1994-2013)

if (file.exists("ensemble.rdata")) {
  load("ensemble.rdata")
} else {
  
  nens = 499
  datmat <- matrix(nrow = nens, ncol = length(varlist))
  colnames(datmat) <- varlist
  
  enslist <- paste("P", formatC(0:(nens-1), width=4, flag="0"), sep="")
  floc <- paste0(ensloc,ensmember,subdir)
  
  for(i in 1:nens){
    
    vec <- rep(NA, length(varlist))
    
    ensmember <- enslist[i] 
    
    fn <- paste0(ensloc,ensmember,'/stats/','JULES-ES-1p0_',ensmember,'_Annual_global.nc')
    #print(fn)
    
    try(nc <- nc_open(paste0(fn)))
    try(vec <- sapply(varlist, FUN = modern_value, nc = nc, ix = 144:164))
    datmat[i, ] <- vec
    nc_close(nc)
  }
  
  save(nens, datmat,enslist,floc, file ="ensemble.rdata")
}
```

```{r}
# Initial clean of data set, removing variables that don't change, and removing NAs (models that didn't run)

nlevel0.ix <- which(is.na(datmat[,3]))

# Y is the whole data set
Y <- datmat
# Y.level0 is the 'cleaned' data set, truncated to variables that change, and removing NAs
Y.level0 <- datmat[-nlevel0.ix, -c(2,30,31)]
Y.nlevel0 <- datmat[nlevel0.ix, -c(2,30,31)]

```


```{r}
# load the original design and input space, normalize to [0-1]

years = 1864:2013
ysec = 60*60*24*365

# Load up the data
lhs_i = read.table('~/brazilCSSP/code/brazil_cssp/analyze_u-ao732/data/ES_PPE_i/lhs_u-ao732.txt', header = TRUE)
lhs_ii = read.table('~/brazilCSSP/code/brazil_cssp/analyze_u-ao732/data/ES_PPE_ii/lhs_u-ao732a.txt', header = TRUE)

toplevel.ix = 1:499

# The raw input data is a latin hypercube
lhs = rbind(lhs_i, lhs_ii)[toplevel.ix, ]
lhs.level0 <- lhs[-nlevel0.ix,]

X = normalize(lhs)
colnames(X) = colnames(lhs)

X.level0 <- X[-nlevel0.ix,]
X.nlevel0 <- X[nlevel0.ix,]

d = ncol(X)
# lower and higher bound on the normalised matrix for visualisation
rx = rbind(rep(0,32), rep(1,32))
```


## Where did the model fail to run?

There are clear run failure thresholds in the parameters rootd_ft_io and lai_max, and quite strong visual indications that a_wl_io and bio_hum_cn matter.

```{r, fig.width = 12, fig.height = 12}

#simple way
par(oma = c(10,10,0,0))
#par(xpd = TRUE)
pairs(X.nlevel0, 
      xlim = c(0,1), ylim = c(0,1),
      col = 'red', 
      gap = 0,
      pch = 20,
      lower.panel = NULL)

#legend('bottomright', legend = paste(1:d, colnames(lhs)), bty = 'n')
legend('left', col = 'red', pch = 20, legend = 'test')

# plot failures over everything else
#X.all <- rbind(X.level0, X.nlevel0)
#colvec <- rep('grey', nrow(X))
#colvec[(nrow(X.level0)+1):nrow(X.all)] <- 'red'
#pairs(X.all, xlim = c(0,1), ylim = c(0,1),col = colvec, gap = 0, lower.panel = NULL, pch = 20, cex = 0.5)

```

## Marginal histograms of run failures

```{r, fig.width = 10, fig.height = 10}

p = ncol(X.level0)

par(mfrow = c(6,6), mar = c(3,2,3,2), fg = 'lightgrey')

for(i in 1:p){
  hist(X.nlevel0[,i], main = colnames(X.nlevel0)[i], xlab = '', ylab = '', col = 'lightgrey', xlim = c(0,1),
       breaks = 10)
}

```




# Histograms of the output at Level 0 (places the model ran and produced output) 

```{r, fig.width = 12, fig.height = 12}
d = ncol(Y.level0)

par(mfrow = c(5,6), fg = 'lightgrey', mar = c(3,2,3,2))

for(i in 1:d){
  hist(Y.level0[,i], main = colnames(Y.level0)[i], xlab = '', ylab = '', col = 'lightgrey')
}

```

## Test out some emulators
At the moment, we'll just keep to the things that we know should work.
We'll use mean NPP as an example

Andy would like to see timeseries of:
cVeg, cSoil and nbp, npp in GtC and GtC/yr.


First, how does NPP respond to each parameter? NAs are removed, but zero values are still included.

```{r, fig.width = 12, fig.height = 12}
p <- ncol(X.level0)

y.level0 <- Y.level0[,'npp_nlim_lnd_sum']

par(mfrow = c(5,7), mar = c(3,1,3,1))
for(i in 1:p){
  plot(X.level0[,i], y.level0, xlab = '', ylab = '', main = colnames(X.level0)[i])
}

```

## A clear threshold in the F0 parameter.
It appears that this ensemble is less "clear cut" in having an output that clearly distinguishes between "failed" (or close to it), and "not failed".

Having said that, having an F0 over a threshold seems to kill the carbon cycle, as before. Here, we've set a threshold of 0.9 (on the normalised scale) for F0, and we remove members of the ensemble with a larger F0 than that when we build emulators.

```{r, fig.width = 6, fig.height = 8}

par(mfrow = c(2,1))
plot(lhs$f0_io, datmat[, 'npp_nlim_lnd_sum'], main = 'Multiplication factor', xlab = 'f0_io', ylab = 'NPP sum')
plot(X.level0[,'f0_io'], Y.level0[, 'npp_nlim_lnd_sum'], xlab = 'f0_io', main = 'Normalized', ylab = 'NPP sum')
abline(v = 0.9)

```



# A DiceKriging emulator for NPP

This emulator is a straight kriging modfel (km) trained with the level 0 data - this includes "zero carbon cycle" but not NAs. The leave-one-out cross validation plot clearly indicates that the emulator over-predicts the carbon cycle when it is very low. This might mean that when constraining, more of the input space is retained than strictly justified. It does suggest that such a constraint is conservative (i.e., it is unlikely that a candidate will be rejected without justifiaction.)
```{r, fig.width = 7, fig.height = 9, results = 'hide'}

em <- km(~., design = X.level0, response = y.level0)
plot(em)

```

This next emulator uses an input dimension reduction technique (glmnet), shrinking regression coefficients of the more unimportant input variables towards zero, before building a kriging emulator with the retained inputs. This doesn't (on the face of it) deal much better with the zero-output ensemble members.

```{r, fig.width = 7, fig.height = 9}

ts.glmnet.em <- twoStep.glmnet(X = X.level0, y = y.level0)
plot(ts.glmnet.em$emulator)

```



# Remove anything with f0 over a threshold (level 1 constraint) .
The level 1 constraint removes any input with F0 greater than 0.9 (normalised), which removes many of the zero-carbon-cycle members up front. There are 424 ensmble members remaining.
```{r}
level1.ix <- which(X.level0[, 'f0_io'] < 0.9)

X.level1 <- X.level0[level1.ix, ]
Y.level1 <- Y.level0[level1.ix,]

y.level1 <- Y.level1[, 'npp_nlim_lnd_sum']
```


```{r, fig.width = 7, fig.height = 7, results = 'hide'}
em.level1 <- km(~., design = X.level1,  response = y.level1)

plot(X.level1[, 'f0_io'], Y.level1[, 'npp_nlim_lnd_sum'], xlab = 'f0_io (normalised)', ylab = 'NPP')
```

Plot the regular km emulator.

```{r, fig.width = 7, fig.height = 7}
plot(em.level1)
```

Plot the "twostep" glmnet/km emulator for the level 1 constraint.

```{r}
ts.glmnet.em.level1 <- twoStep.glmnet(X = X.level1, y = y.level1)
```


```{r, fig.width = 7, fig.height = 9}
plot(ts.glmnet.em.level1$emulator )

```


## A one-at-a-time sensitivity analysis of the "sum" output 
```{r}

if (file.exists("oaat.rdata")) {
  load("oaat.rdata")
} else {
  
  yvec <- c('nbp_lnd_sum', 'fLuc_lnd_sum', 'npp_nlim_lnd_sum', 'cSoil_lnd_sum',
            'cVeg_lnd_sum', 'landCoverFrac_lnd_sum', 'fHarvest_lnd_sum',
            'lai_lnd_sum', 'rh_lnd_sum', 'treeFrac_lnd_sum', 'c3PftFrac_lnd_sum', 
            'c4PftFrac_lnd_sum', 'shrubFrac_lnd_sum', 'baresoilFrac_lnd_sum')
  
  oat.var.sensmat <- matrix(NA, nrow = length(yvec), ncol = ncol(X.level1))
  
  for(i in 1:length(yvec)){
    
    yname <- yvec[i]
    y <- Y.level1[, yname]
    loo <- twoStep.sens(X = X.level1, y = y)
    oat.var.sensmat[i, ] <- loo 
  }
  
  save(yvec, oat.var.sensmat, file = "oaat.rdata")
  
}
```


## Heatmaps of one-at-a-time sensitivity analysis across a number of variables

```{r, fig.width= 10, fig.height = 6}
# normalize the sensitivity matrix
colnames(oat.var.sensmat) <- colnames(X.level1)
rownames(oat.var.sensmat) <- yvec

#test <- normalize(t(oat.var.sensmat))

#image(test)
#par()
heatmap(oat.var.sensmat, Rowv = NA, Colv = NA, mar = c(10,10), scale = 'row')
heatmap(oat.var.sensmat, mar = c(10,10), scale = 'row')

```

```{r, fig.width =12, fig.height = 7}

normsens <- normalize(t(oat.var.sensmat))

par(mar = c(10,12,5,1))
image(normsens, axes = FALSE, col = blues)
axis(1, at = seq(from = 0, to = 1, length.out = p), labels = colnames(X.level1), las = 2)
axis(2, at = seq(from = 0, to = 1, length.out = length(yvec)), labels = yvec, las = 1)
mtext('One-at-a-time sensitivity, variance across level 1 ensemble', side = 3, adj = 0, line = 2, cex = 1.8)

```


## How good are the emulators for each output?
The emulators appear to be at least capturing the broad response for all of the output variables.

First, plot the straight kriging emulators
```{r, fig.width = 12, fig.height = 6, results = 'hide'}

if (file.exists("km_emulators.rdata")) {
  load("km_emulators.rdata")
} else {
  
  emlist.km <- vector(mode = 'list', length = length(yvec))
  #for(i in 1:length(yvec)){
  for(i in 1:length(yvec)){
    yname <- yvec[i]
    y <- Y.level1[, yname]
    
    em <- km(~., design = X.level1, response = y)
    emlist.km[[i]] <- em
  }
  
  loolist.km <- vector(mode = 'list', length = length(yvec))
  
  for(i in 1:length(yvec)){
    
    loo <- leaveOneOut.km(model = emlist.km[[i]], type = 'UK', trend.reestim = TRUE)
    loolist.km[[i]] <- loo
  }
  
  save(emlist.km,loolist.km, file = "km_emulators.rdata")
}

```

```{r, fig.width = 12, fig.height = 12}
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,0.1,0.1))
for(i in 1:length(loolist.km)){
  
  y <- Y.level1[, yvec[i]]
  loo <- loolist.km[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = colnames(Y.level1)[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 

```

Next, plot the twostep glmnet/km emulators

```{r, fig.width = 12, fig.height = 6}
# Twostep glmnet emulators

if (file.exists("ts_emulators.rdata")) {
  load("ts_emulators.rdata")
} else {
  
  emlist.twoStep.glmnet <- vector(mode = 'list', length = length(yvec))
  for(i in 1:length(yvec)){
    
    yname <- yvec[i]
    y <- Y.level1[, yname]
    
    em <- twoStep.glmnet(X = X.level1, y = y)
    emlist.twoStep.glmnet[[i]] <- em
  }
  
  loolist.twoStep.glmnet <- vector(mode = 'list', length = length(yvec))
  
  for(i in 1:length(yvec)){
    
    loo <- leaveOneOut.km(model = emlist.twoStep.glmnet[[i]]$emulator, type = 'UK', trend.reestim = TRUE)
    loolist.twoStep.glmnet[[i]] <- loo
  }
  
  save(emlist.twoStep.glmnet, loolist.twoStep.glmnet, file = "ts_emulators.rdata")
}

```


```{r, fig.width = 12, fig.height = 12}
# plot the glmnet emulators

par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,0.1,0.1))
for(i in 1:length(loolist.twoStep.glmnet )){
  
  y <- Y.level1[, yvec[i]]
  loo <- loolist.twoStep.glmnet[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = colnames(Y.level1)[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2)    

```



# Constrain input space with a small number of observations

We use thresholds of tolerance from Andy on 'nbp_lnd_sum', 'npp_nlim_lnd_sum', 'cSoil_lnd_sum', 'cVeg_lnd_sum' as basic initial constraints on the input space.
```{r, fig.width = 8, fig.height = 8}
ynames.const <- c('nbp_lnd_sum', 'npp_nlim_lnd_sum', 'cSoil_lnd_sum', 'cVeg_lnd_sum')
yunits.const <- c('GtC/year', 'GtC/year', 'GtC', 'GtC')
Y.const.level1 <- Y.level1[, ynames.const]

scalevec <- c(1e12/ysec, 1e12/ysec, 1e12, 1e12)
Y.const.level1.scaled <- sweep(Y.const.level1,2, STATS = scalevec, FUN = '/' )


level2.ix = which(Y.const.level1.scaled[,'nbp_lnd_sum'] > -10 &
                    Y.const.level1.scaled[,'npp_nlim_lnd_sum'] > 35 &  Y.const.level1.scaled[,'npp_nlim_lnd_sum'] < 80 &
                    Y.const.level1.scaled[,'cSoil_lnd_sum'] > 750 & Y.const.level1.scaled[,'cSoil_lnd_sum'] < 3000 &
                  Y.const.level1.scaled[,'cVeg_lnd_sum'] > 300 & Y.const.level1.scaled[,'cVeg_lnd_sum'] < 800
  )
```


```{r, fig.width = 10, fig.height = 7}

# Histogram of level 1 constraints
hcol = 'darkgrey'
lcol = 'black'
par(mfrow = c(2,2), fg = 'darkgrey', las = 1)



hist(Y.const.level1.scaled[,'nbp_lnd_sum'], col = hcol, main = 'NBP', xlab = 'GtC/year')
polygon(x = c(-10, 100, 100, -10), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
        border = makeTransparent('tomato2'))
hist(Y.const.level1.scaled[,'cSoil_lnd_sum'], col = hcol, main = 'Soil Carbon', xlab = 'GtC')
polygon(x = c(750, 3000, 3000, 750), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
        border = makeTransparent('tomato2'))

hist(Y.const.level1.scaled[,'cVeg_lnd_sum'], col = hcol, main = 'Vegetation Carbon', xlab = 'GtC')
polygon(x = c(300, 800, 800, 300), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
       border =  makeTransparent('tomato2'))
hist(Y.const.level1.scaled[,'npp_nlim_lnd_sum'], col = hcol , main = 'NPP', xlab = 'GtC/year')
polygon(x = c(35, 80, 80, 35), y = c(0, 0, 1000, 1000),
        col = makeTransparent('tomato2'),
        border = makeTransparent('tomato2')
)

```


## Ensemble members that comply with constraints in NPP, NBP, soil and vegetation carbon.

```{r, fig.width = 12, fig.height = 12}

X.const <- X.level1[ level2.ix , ]
pairs(X.const, xlim = c(0,1), ylim = c(0,1), gap = 0, pch = 20, lower.panel = NULL)

```

## Build a two-step glmnet/km emulator for each output that we have constraints for, and find the input space where those constraints are met.

```{r}

if (file.exists("constraints.rdata")) {
  load("constraints.rdata")
} else {
  
  
  mins <- apply(X.level1,2,FUN = min)
  maxes <- apply(X.level1,2,FUN = max)
  
  nsamp.unif = 100000  
  X.unif = samp.unif(nsamp.unif, mins = mins, maxes = maxes)
  
  Y.unif = matrix(nrow = nsamp.unif, ncol = ncol(Y.const.level1.scaled))
  
  colnames(Y.unif) = colnames(Y.const.level1.scaled)
  
  emlist.const <- vector(mode = 'list', length = ncol(Y.const.level1.scaled))
  
  # Build an emulator for each output individually
  for(i in 1:ncol(Y.unif)){
    em <- twoStep.glmnet(X = X.level1, y = Y.const.level1.scaled[,i])
    emlist.const[[i]] <- em
    pred = predict(em$emulator, newdata = X.unif, type = 'UK')
    Y.unif[,i] <- pred$mean
  }
  
  save(emlist.const, X.unif, nsamp.unif, Y.unif, mins, maxes, file = "constraints.rdata")
  
}


# Find the inputs where the mean of the emulated output is 
# within the tolerable limits set by the modeller.

level2.unif.kept.ix = which(Y.unif[,'nbp_lnd_sum'] > -10 &
                    Y.unif[,'npp_nlim_lnd_sum'] > 35 &  Y.unif[,'npp_nlim_lnd_sum'] < 80 &
                    Y.unif[,'cSoil_lnd_sum'] > 750 & Y.unif[,'cSoil_lnd_sum'] < 3000 &
                  Y.unif[,'cVeg_lnd_sum'] > 300 & Y.unif[,'cVeg_lnd_sum'] < 800
  )


X.kept = X.unif[level2.unif.kept.ix , ]

# we've removed 80% of our prior input space
print(paste0('NROY space proportion (%) = ',(nrow(X.kept) / nsamp.unif) * 100))

ix.rejected = setdiff(1:nsamp.unif, level2.unif.kept.ix)
X.rejected = X.unif[ix.rejected, ]


```


## Pairs plot of input space that is Not Ruled Out Yet when basic constraints are applied to annual data.

```{r emulator_pairs, fig.width = 10, fig.height = 10, warning = FALSE, message = FALSE}

par(oma = c(0,0,0,3), bg = 'white')
pairs(X.kept,
      labels = 1:d,
      gap = 0, lower.panel = NULL, xlim = c(0,1), ylim = c(0,1),
      panel = dfunc.up,
      cex.labels = 1,
      col.axis = 'white',
      dfunc.col = blues)

image.plot(legend.only = TRUE,
           zlim = c(0,1),
           col = blues,
           legend.args = list(text = 'Density of model runs matching the criteria', side = 3, line = 1),
           horizontal = TRUE
)

legend('left', legend = paste(1:d, colnames(lhs)), cex = 0.9, bty = 'n')

```

# Marginal histograms of space that is Not Ruled Out Yet when basic constraints are applied.

```{r, fig.width = 12, fig.height = 12}

par(mfrow = c(8,4), fg = 'grey', mar = c(3,2,2,1))

for(i in 1:ncol(X.unif)){
  
  hist(X.kept[,i], col = 'grey', main = colnames(X.unif)[i], xlim = c(0,1),axes = FALSE, xlab = '', ylab = '')
  axis(1)
  
}



```



# Monte Carlo Filtering for sensitivity analysis.
Monte Carlo filtering (MCF) gives another form of sensitivity metric. MCF splits the input sample into two parts - those that do and do not meet some threshold, for example, and then examines the differences of the resulting parameter distributions. A useful guide to MCF can be found in [Pianosi et al (2016)]<https://www.sciencedirect.com/science/article/pii/S1364815216300287>

We use the emulated input samples that are ruled out and NROY with the initial constraint.

```{r, message = FALSE, results = 'hide', warning = FALSE}

# ---------------------------------------------------------------------------------
# Monte carlo filtering for sensitivity analysis
# ---------------------------------------------------------------------------------

# Uniform sample from across parameter space
# Split the sample into 'behavioural' (NROY) and 'Non behavioural (Ruled Out)
# Build cdfs of the marginal distributions in each case
# Perform a KS test to see if the smaples are drawn from different distributions
# The KS statistic is an indicator of the importance of the parameter in splitting the
# samples.

# "Not in" function
'%!in%' <- function(x,y)!('%in%'(x,y))

mcf = function(X, nroy.ix){

  ## Monte Carlo Filtering function
  ## X   ............... Complete sample from input space
  ## nroy.ix ........... index of cases of X which are NROY (Not Ruled Out Yet), or 'behavioural'.

  ## produces ks statistic for each column of the input matrix X
  ## A larger ks statistic means that input is more important for
  ## determining if a sample is NROY or not

  X.nroy = X[nroy.ix, ]

  ref = 1:nrow(X)
  ro.ix = which(ref %!in% nroy.ix)
  X.ro = X[ro.ix, ]

  kss = rep(NA, length = ncol(X))
  for(i in 1:ncol(X)){

    ks = ks.test(X.ro[,i], X.nroy[,i])
    kss[i] = ks$statistic

  }

  out = kss
  out
}

mcf.level2 = mcf(X.unif,level2.unif.kept.ix)

```

```{r, fig.width=12, fig.height=6}


mcf.level2.sort <- sort(mcf.level2, decreasing = TRUE, index.return = TRUE )

par(mar = c(10,5,3,2))
plot(mcf.level2.sort$x, axes = FALSE, xlab = '', ylab = 'MCF score', pch = 19)
segments(x0 = 1:p, y0 = 0, x1 = 1:p, y1 = mcf.level2.sort$x)
axis(1, at  = 1:p, labels = colnames(X.level1)[mcf.level2.sort$ix], las = 2)
axis(2, las = 1)
mtext('Monte Carlo Filtering sensitivity (ordered)', side = 3, adj = 0, line = 1, cex = 1.5)

```






