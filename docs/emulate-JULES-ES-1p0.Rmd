---
title: "Testing emulators of the Earth System configuration of JULES"
author: "Doug McNeall"
date: "09/08/2021"
output: 
    html_notebook:
        toc: true
        toc_float: true
        toc_depth: 3
        number_sections: true
---

## Preliminaries
Load libraries, functions and data.

```{r, echo = FALSE, message = FALSE, warning=FALSE, results = 'hide'}
# Load helper functions

knitr::opts_chunk$set(fig.path = "figs/", echo = FALSE, message = FALSE, warnings = FALSE)

# load helper functions, data and do preliminary processing of the ensemble.
source('JULES-ES-1p0-common.R')

```

## Specific helper functions
```{r}

mat2list <- function(X){
  
  # Turns the p columns of a matrix into a p length list,
  # with each column becoming an element of the list
  
  out <- vector(mode = 'list', length = ncol(X))
  for(i in 1:ncol(X)){
    out[[i]] <- X[ , i]
  }
  out
  
}


# How about MAE over the range
prop_mae <- function(Y, Ypred){
  # mean absolute error as a proportion of the range of output
  
  absdiff <- abs(diff(range(Y)))
  
  mae <- MAE(Y, Ypred)
  
  propmae <- (mae / absdiff) * 100
  
  propmae
  
}



withinConstraints <- function(X, Xrange){
  # return the index of a matrix that conforms to constraints.  
  
  # X         ........  Matrix to be tested
  # Xrange    ........ Range matrix. Each column has min value in row 1 and max value in row 2
   
  
  kept_list <-vector(mode = 'list', length = ncol(X))
  
  for(i in 1:ncol(X)){

    kept_ix <- which(X[ ,i] > Xrange[1, i] & X[ ,i] < Xrange[2, i])
    
    kept_list[[i]] <- kept_ix
  }
  
  out <- kept_list[[1]]
  # run along the list and just keep the intersection
  # with each iteration
  for(i in 1:length(kept_list)){
    
    out <- intersect(out, kept_list[[i]])
  }
  
  out
  
}

```

```{r}
AW_const <- matrix(c(0, 1e12, 35, 80, 750, 3000, 300, 800), byrow = FALSE, nrow = 2)
```


## Leave-one-out analysis of emulators

The leave-one-out section in a for loop can be replaced by using lapply (or mclapply for speed).  

First, build a list of emulators and then perform a leave-one-out. 
```{r, include = FALSE}
# include = FALSE should run the code but supress all output.


Y_sum_level1a <- Y_level1a[ , y_names_sum]
YAnom_sum_level1a <- YAnom_level1a[ , y_names_sum]


Y_sum_level1a_list <- mat2list(Y_sum_level1a)
YAnom_sum_level1a_list <- mat2list(YAnom_sum_level1a)
```


Build the emulator lists
```{r, include = FALSE}
# Here, the list is a list version of the matrix Y_
emlist_km_Y_level1a <- mclapply(X = Y_sum_level1a_list, FUN = km, formula = ~., design = X_level1a, mc.cores = 4) 

```

```{r, include = FALSE}
emlist_km_YAnom_level1a <- mclapply(X = YAnom_sum_level1a_list, FUN = km, formula = ~., design = X_level1a, mc.cores = 4) 
```


Apply the leave-one-out test to each of the list members.

```{r}

loolist_km_Y_level1a <- mclapply(X = emlist_km_Y_level1a, FUN = leaveOneOut.km, type = 'UK', trend.reestim = TRUE)

loolist_km_YAnom_level1a <- mclapply(X = emlist_km_YAnom_level1a, FUN = leaveOneOut.km, type = 'UK', trend.reestim = TRUE)

```


```{r}
loostats_km_Y_level1a <- lapply(emlist_km_Y_level1a, FUN = kmLooStats)
loostats_km_YAnom_level1a <- lapply(emlist_km_YAnom_level1a, FUN = kmLooStats)

```

```{r, fig.width = 12, fig.height = 12}

#pdf(file = 'figs/kmloostats_Y_level1a.pdf', width = 12, height = 12)
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,4,0.1))
for(i in 1:length(loolist_km_Y_level1a)){
  
  y <- Y_level1a[, y_names_sum[i]]
  loo <- loolist_km_Y_level1a[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = '' , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)
  legend('topleft', legend = y_names_sum[i], bty = 'n', text.font = 2  )
  legend('bottomright',legend = paste('pmae =',round(loostats_km_Y_level1a[[i]]$pmae,2),'%') , bty = 'n', text.font = 2)

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 
mtext('Level 1a ensemble outputs', side = 3, line = 0, outer = TRUE, cex = 2)

#dev.off()

```

```{r, fig.width = 12, fig.height = 12}

#pdf(file = 'figs/kmloostats_YAnom_level1a.pdf', width = 12, height = 12)
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,4,0.1))
for(i in 1:length(loolist_km_YAnom_level1a)){
  
  y <- YAnom_level1a[, y_names_sum[i]]
  loo <- loolist_km_YAnom_level1a[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = '' , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)
  legend('topleft', legend = y_names_sum[i], bty = 'n', text.font = 2  )
  legend('bottomright',legend = paste('pmae =',round(loostats_km_YAnom_level1a[[i]]$pmae,2),'%') , bty = 'n', text.font = 2)

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 
mtext('Level 1a ensemble outputs', side = 3, line = 0, outer = TRUE, cex = 2)

#dev.off()

```


## How well do we predict the constrained members?

```{r}
# Identify all of the members which fall under a "level 2" constraint.
level2_ix <- withinConstraints(Y_const_level1a_scaled, AW_const)

# Build an emulator for the scaled output

Y_const_level1a_scaled_list <- mat2list(Y_const_level1a_scaled)

emlist_km_Y_const_level1a_scaled <- mclapply(X = Y_const_level1a_scaled_list, FUN = km, formula = ~., design = X_level1a, mc.cores = 4) 

```

```{r}

loolist_km_Y_const_level1a_scaled  <- mclapply(X = emlist_km_Y_const_level1a_scaled, FUN = leaveOneOut.km, type = 'UK', trend.reestim = TRUE)

```


```{r}
# Would the prediction be in the constrained?


pred_km_Y_const_level1a_scaled <- matrix(ncol = ncol(Y_const_level1a_scaled), nrow = nrow(Y_const_level1a_scaled))
colnames(pred_km_Y_const_level1a_scaled) <- colnames(Y_const_level1a_scaled)


for(i in 1:ncol(Y_const_level1a_scaled)){
  
  pred <- loolist_km_Y_const_level1a_scaled[[i]]$mean
  pred_km_Y_const_level1a_scaled[, i] <- pred
  
}

pred_level2_ix <- withinConstraints(pred_km_Y_const_level1a_scaled, AW_const)

```

Plotting the predictions, with a special focus on the constrained members. 
The message is, to do better on predicting the members, we need to do better with the cVeg emulator. Many of its members are way too low, and this is dragging down the predictions. Some are false positives though.
Could we plot Loo error vs the parameters?

```{r, fig.width = 10, fig.height = 10}

# True positive is things in both observed and predicted.

tp_ix <- intersect(level2_ix, pred_level2_ix)

# False positive is things in predicted but not in observed.
fp_ix <- setdiff(pred_level2_ix, level2_ix)

# False negative is things in observed but not predicted
fn_ix <- setdiff(level2_ix, pred_level2_ix)

# true negative is things not in observed or predicted

tn_ix <- setdiff(1:nrow(Y_const_level1a_scaled), union(level2_ix, pred_level2_ix))

#should be 362
#length(c(tp_ix, fp_ix, fn_ix, tn_ix))

clines_lower <- c(0, 35, 750, 300)
clines_upper <- c(NA, 80, 3000, 800)

# correctly predicted in constrained group = red
colvec <- rep('black', nrow(Y_const_level1a_scaled))
pchvec <- rep(21, nrow(Y_const_level1a_scaled))

colvec[tp_ix] <- 'blue'
pchvec[tp_ix] <- 19

colvec[fp_ix] <- 'red'
pchvec[fp_ix] <- 19

colvec[fn_ix] <- 'gold'
pchvec[fn_ix] <- 19

colvec[tn_ix] <- 'darkgrey'
pchvec[tn_ix] <- 21
  

#pdf(width = 10, height = 10, file = 'figs/Y_const_loo.pdf')
par(mfrow = c(2,2), oma = c(0.1,0.1,4,0.1))
for(i in 1:4){
plot(Y_const_level1a_scaled[ ,i], pred_km_Y_const_level1a_scaled[ ,i], type = 'n', las = 1, main = colnames(Y_const_level1a_scaled)[i],
     xlab = 'model', ylab = 'emulator')
  
abline(0,1)
abline(v = clines_lower[i], col = 'darkgrey')
abline(v = clines_upper[i], col = 'darkgrey', lty = 'dashed')

abline(h = clines_lower[i], col = 'darkgrey')
abline(h = clines_upper[i], col = 'darkgrey', lty = 'dashed')

points(Y_const_level1a_scaled[ ,i], pred_km_Y_const_level1a_scaled[ ,i], col = colvec, pch = pchvec)

}

reset()

legend('top', legend = c('True positive', 'False Positive', 'False Negative', 'True Negative', 'lower bound', 'upper bound'), pch = c(19,19, 19, 21, NA, NA), col = c('blue', 'red', 'gold', 'darkgrey', 'darkgrey', 'darkgrey'), lty = c(NA,NA,NA,NA, 'solid', 'dashed'), horiz = TRUE)

#dev.off()

```


```{r}
library(verification)


model <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))

model[level2_ix] <- TRUE
emulator[pred_level2_ix] <- TRUE

ver <- verify(obs = model, pred = emulator, frcst.type = 'binary')

# ETS runs from -1/3 to 1, with 0 showing no skill, so we have some skill. Could be better.
ver$ETS
ver$HSS

```

## A better emulator for cVeg
It's clear from other experiments that a major barrier to a good prediction of a constrained is a better emulator for cVeg.

Some experiments to create a better emulator

```{r}
# more multistarts


y <- Y_const_level1a_scaled[, 'cVeg_lnd_sum']
# change the output

# This breaks
#m2 <- km(~.^2, design=X_level1a, response=y, multistart = 4)


```



```{r}
require(foreach)

# below an example for a computer with 2 cores, but also work with 1 core

nCores <- 4
require(doParallel)
cl <-  makeCluster(nCores) 
registerDoParallel(cl)

# kriging model 1, with 4 starting points 
m_stan <- km(~., design=X_level1a, response = y, multistart=4)
m_logy <- km(~., design=X_level1a, response = log(y), multistart=4)
m_sqrty <- km(~., design=X_level1a, response = sqrt(y), multistart=4)


stopCluster(cl)


```

```{r}
loo_m_stan <- leaveOneOut.km(m_stan, type = 'UK', trend.reestim = TRUE)

loo_m_logy <- leaveOneOut.km(m_logy, type = 'UK', trend.reestim = TRUE)

loo_m_sqrty <- leaveOneOut.km(m_sqrty, type = 'UK', trend.reestim = TRUE)


```

How good are the leave-one-out predictions of the transformed data?

It appears the square root transformation outperforms the standard - but mostly at low values (which we don't think are realistic).

```{r}

plot(y, loo_m_stan$mean,col = makeTransparent('black', 150), pch = 19)
points(y, exp(loo_m_logy$mean), col = makeTransparent('red', 150), pch = 19)
points(y, (loo_m_sqrty$mean)^2, col = makeTransparent('blue', 150), pch = 19)
abline(0,1)
abline(h = c(300, 800))
abline(v = c(300, 800))


errSummary <- function(obs, pred){
  
    err <- pred - obs
    mae <- mean(abs(err))
    rmse <- sqrt(mean(err^2))
    maxerr <- max(err)
    absdiff <- abs(diff(range(obs)))
    pmae <- (mae/absdiff) * 100
    return(list(mae = mae, pmae = pmae, maxerr = maxerr))
}


errSummary(y, loo_m_stan$mean)

errSummary(y, exp(loo_m_logy$mean))

errSummary(y, (loo_m_sqrty$mean)^2)


```


```{r}
# Make a new matrix of predictions, and find which enemble members would pass the constraint.

pred_km_Y_const_level1a_scaled_cVeg_sqrty <- pred_km_Y_const_level1a_scaled
pred_km_Y_const_level1a_scaled_cVeg_sqrty[, 'cVeg_lnd_sum'] <- (loo_m_sqrty$mean)^2

pred_level2_sqrty_ix <- withinConstraints(pred_km_Y_const_level1a_scaled_cVeg_sqrty, AW_const)

emulator_sqrty <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator_sqrty[pred_level2_sqrty_ix] <- TRUE
ver_sqrty<- verify(obs = model, pred = emulator_sqrty, frcst.type = 'binary')
```


```{r}
# Make a new matrix of predictions, and find which enemble members would pass the constraint.

pred_km_Y_const_level1a_scaled_cVeg_logy <- pred_km_Y_const_level1a_scaled
pred_km_Y_const_level1a_scaled_cVeg_logy[, 'cVeg_lnd_sum'] <- exp(loo_m_logy$mean)

pred_level2_logy_ix <- withinConstraints(pred_km_Y_const_level1a_scaled_cVeg_logy, AW_const)


emulator_logy <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator_logy[pred_level2_logy_ix] <- TRUE
ver_logy <- verify(obs = model, pred = emulator_logy, frcst.type = 'binary')
# ETS runs from -1/3 to 1, with 0 showing no skill, so we have some skill. Could be better.
```


```{r}
ver$ETS
ver$HSS

ver_sqrty$ETS
ver_sqrty$HSS


ver_logy$ETS
ver_logy$HSS

```


```{r}
ver$tab

ver_logy$tab

ver_sqrty$tab


```


```{r}

nCores <- 4
require(doParallel)
cl <-  makeCluster(nCores) 
registerDoParallel(cl)

# kriging model 1, with 4 starting points 
m_stan_gauss <- km(~., design=X_level1a, response = y, multistart=4, covtype = 'gauss' )
loo_m_stan_gauss <- leaveOneOut.km(m_stan_gauss , type = 'UK', trend.reestim = TRUE)
plot(y, loo_m_stan_gauss$mean)


m_stan_gen <- km(~., design=X_level1a, response = y, optim.method = 'gen' )
loo_m_stan_gen <- leaveOneOut.km(m_stan_gen, type = 'UK', trend.reestim = TRUE)
plot(y, loo_m_stan_gen$mean)


m_stan_gauss_gen <- km(~., design=X_level1a, response = y, optim.method = 'gen' )
loo_m_stan_gauss_gen <- leaveOneOut.km(m_stan_gauss_gen, type = 'UK', trend.reestim = TRUE)
plot(y, loo_m_stan_gauss_gen$mean)


m_logy_gauss_gen <- km(~., design=X_level1a, response = log(y), covtype = 'gauss', optim.method = 'gen')
loo_m_logy_gauss_gen <- leaveOneOut.km(m_logy_gauss_gen, trend.reestim = TRUE, type = 'UK')
plot(y, exp(loo_m_logy_gauss_gen$mean))


m_sqrt_gauss_gen <- km(~., design=X_level1a, response = sqrt(y), covtype = 'gauss', optim.method = 'gen')
loo_m_sqrt_gauss_gen <- leaveOneOut.km(m_sqrt_gauss_gen, trend.reestim = TRUE, type = 'UK')
plot(y, (loo_m_sqrt_gauss_gen$mean)^2)


stopCluster(cl)


```

```{r}
plot(y, loo_m_stan_gauss_gen$mean)
abline(0,1)
```


```{r}
plot(y, (loo_m_sqrt_gauss_gen$mean)^2)
abline(0,1)
```


```{r}
pred_km_Y_const_level1a_scaled_cVeg_logy_gauss_gen <- pred_km_Y_const_level1a_scaled
pred_km_Y_const_level1a_scaled_cVeg_logy_gauss_gen[, 'cVeg_lnd_sum'] <- exp(loo_m_logy_gauss_gen$mean)

pred_level2_logy_gauss_gen_ix <- withinConstraints(pred_km_Y_const_level1a_scaled_cVeg_logy_gauss_gen, AW_const)


emulator_logy_gauss_gen <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator_logy_gauss_gen[pred_level2_logy_gauss_gen_ix] <- TRUE
ver_logy_gauss_gen <- verify(obs = model, pred = emulator_logy_gauss_gen, frcst.type = 'binary')
```





```{r}

pred_km_Y_const_level1a_scaled_cVeg_logy_gauss_gen <- pred_km_Y_const_level1a_scaled
pred_km_Y_const_level1a_scaled_cVeg_logy_gauss_gen[, 'cVeg_lnd_sum'] <- exp(loo_m_logy_gauss_gen$mean)

pred_level2_logy_gauss_gen_ix <- withinConstraints(pred_km_Y_const_level1a_scaled_cVeg_logy_gauss_gen, AW_const)


emulator_logy_gauss_gen <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator_logy_gauss_gen[pred_level2_logy_gauss_gen_ix] <- TRUE
ver_logy_gauss_gen <- verify(obs = model, pred = emulator_logy_gauss_gen, frcst.type = 'binary')


```



```{r}
pred_km_Y_const_level1a_scaled_cVeg_stan_gauss_gen <- pred_km_Y_const_level1a_scaled
pred_km_Y_const_level1a_scaled_cVeg_stan_gauss_gen[, 'cVeg_lnd_sum'] <- (loo_m_stan_gauss_gen$mean)

pred_level2_stan_gauss_gen_ix <- withinConstraints(pred_km_Y_const_level1a_scaled_cVeg_stan_gauss_gen, AW_const)


emulator_stan_gauss_gen <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator_stan_gauss_gen[pred_level2_stan_gauss_gen_ix] <- TRUE
ver_stan_gauss_gen <- verify(obs = model, pred = emulator_stan_gauss_gen, frcst.type = 'binary')
```

 
# How about truncating the design? 

(I don't think I have this right yet.)
 
```{r}

DM_const <- AW_const <- matrix(c(0, 1e12, 10, 120, 500, 5000, 100, 1000), byrow = FALSE, nrow = 2)

level1b_ix <- withinConstraints(Y_const_level1a_scaled, DM_const) 

X_level1b <- X_level1a[level1b_ix , ]

y_level1b <- Y_const_level1a_scaled[level1b_ix, 'cVeg_lnd_sum']

m_stan_level1b <- km(~., design = X_level1b, response = y_level1b )

loo_m_stan_level1b <- leaveOneOut.km(m_stan_level1b, type = 'UK', trend.reestim = TRUE)

pred_km_Y_const_level1a_scaled_cVeg_stan_level1b <- pred_km_Y_const_level1a_scaled[level1b_ix, ]
pred_km_Y_const_level1a_scaled_cVeg_stan_level1b[, 'cVeg_lnd_sum'] <- (loo_m_stan_level1b$mean)

pred_level2_stan_level1b_ix <- withinConstraints(pred_km_Y_const_level1a_scaled_cVeg_stan_level1b, AW_const)


emulator_stan_level1b <- vector(mode = "logical", length = nrow(Y_const_level1a_scaled))
emulator_stan_level1b[pred_level2_stan_level1b_ix] <- TRUE
ver_stan_level1b <- verify(obs = model, pred = emulator_stan_level1b, frcst.type = 'binary')

```
 
## Modelling emulator error in cVeg

```{r}
cVeg_level1a <- Y_const_level1a_scaled[, 'cVeg_lnd_sum']


require(foreach)

# below an example for a computer with 2 cores, but also work with 1 core

nCores <- 4
require(doParallel)
cl <-  makeCluster(nCores) 
registerDoParallel(cl)

m_cVeg_stan <- km(~., design = X_level1a, response = cVeg_level1a, multistart = 4   )

stopCluster(cl)

```

First, plot and emulate the value of cVeg
```{r}
mins <- apply(X_level1a,2,FUN = min)
maxes <- apply(X_level1a,2,FUN = max)
  
nsamp_unif <- 1000  
X_unif <- samp_unif(nsamp_unif, mins = mins, maxes = maxes)

pred_cVeg_stan_unif <- predict(m_cVeg_stan, newdata = X_unif, type = 'UK')
```

There is a really important threshold for cVeg in b_wl_io. This is actually picked up nicely in the History matching section, so I'm not worried that it's being missed.

```{r, fig.width = 12, fig.height = 12, warning= FALSE, mesaage = FALSE}

cVeg_level1a_trunc <- cVeg_level1a
cVeg_level1a_trunc[cVeg_level1a > 800] <- 800
par(oma = c(0,0,0,3), bg = 'white')
pairs(X_level1a,
      labels = 1:d,
      gap = 0, lower.panel = NULL, xlim = c(0,1), ylim = c(0,1),
      panel = cpoints,
      z = cVeg_level1a_trunc,
      col = byr,
      cex.labels = 1,
      col.axis = 'white',
      pch = 20
      )

image.plot(legend.only = TRUE,
           zlim = range(cVeg_level1a_trunc),
           col = byr,
           legend.args = list(text = 'cVeg', side = 3, line = 1),
           horizontal = TRUE
)

  legend('left', legend = paste(1:d, colnames(lhs)), cex = 0.9, bty = 'n')

```


```{r, fig.width = 10, fig.height = 10}
par(mfrow = c(2,2))
plot(X_level1a[, 'b_wl_io'], cVeg_level1a)
plot(X_level1a[, 'kaps_roth'], cVeg_level1a)
plot(X_level1a[, 'lai_max_io'], cVeg_level1a)
plot(X_level1a[, 'retran_l_io'], cVeg_level1a)

```

Then use the leave-one-out differences to get an idea of parameters where the emulator is going wrong. (these should be roughly the same)

```{r}

```




### Comparison of level 0 and level 1 emulators for NPP (modern value)

```{r, fig.width = 7, fig.height = 7, results = 'hide', warning=FALSE}
tic()
em_npp_level0 <- km(~., design = X_level0,  response = y_level0)
toc()
```


```{r, fig.width = 7, fig.height = 7, results = 'hide', warning=FALSE}
em_npp_level1 <- km(~., design = X_level1,  response = y_level1)

em_npp_level0_ms <- km(~., design = X_level0,  response = y_level0, multistart = 4)
em_npp_level1_ms <- km(~., design = X_level1,  response = y_level1, multistart = 4)


```


 # Plot the regular km emulator. Doesn't look great.

```{r plot-em-npp-level0, fig.width = 7, fig.height = 7}
plot(em_npp_level0)
```

```{r plot-em-npp-level1, fig.width = 7, fig.height = 7}
plot(em_npp_level1)
```


### Leave-one-out summaries of NPP emulators
```{r}

loo_npp_level0 <- leaveOneOut.km(em_npp_level0, type = 'UK', trend.reestim = TRUE)
loo_npp_level1 <- leaveOneOut.km(em_npp_level1, type = 'UK', trend.reestim = TRUE)

RMSE ( loo_npp_level0$mean, y_level0)
RMSE ( loo_npp_level1$mean, y_level1)



prop_mae(y_level0, loo_npp_level0$mean)

prop_mae(y_level1, loo_npp_level1$mean)






```



```{r}

# It doesn't look like Multistart makes a big difference at all for NPP
errstats_npp_level0 <- kmLooStats(km = em_npp_level0)
errstats_npp_level0_ms <- kmLooStats(km = em_npp_level0_ms)

errstats_npp_level1 <- kmLooStats(km = em_npp_level1)
errstats_npp_level1_ms <- kmLooStats(km = em_npp_level1_ms)


# proportional mean absolute error
errstats_npp_level0$pmae
errstats_npp_level0_ms$pmae


errstats_npp_level1$pmae
errstats_npp_level1_ms$pmae




```



## How good are the emulators for each output?
The emulators appear to be at least capturing the broad response for all of the output variables.

First, plot the straight kriging emulators

```{r, fig.width = 12, fig.height = 6, results = 'hide'}

if (file.exists("km_emulators_Y.rdata")) {
  load("km_emulators_Y.rdata")
} else {
  
  emlist_km_Y <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    yname <- y_names_sum[i]
    y <- Y_level1[, yname]
    
    em <- km(~., design = X_level1, response = y)
    emlist_km_Y[[i]] <- em
  }
  
  loolist_km_Y <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    
    loo <- leaveOneOut.km(model = emlist_km_Y[[i]], type = 'UK', trend.reestim = TRUE)
    loolist_km_Y[[i]] <- loo
  }
  
  save(emlist_km_Y,loolist_km_Y, file = "km_emulators_Y.rdata")
}

```





```{r}
#tic()
#fit_list_Y_sum_level1a <- createKmFitList(X = X_level1a, Y = Y_sum_level1a)
#toc(log = TRUE)

#tic()
#fit_list_Y_sum_level1a_par <- createKmFitListParallel(X = X_level1a, Y = Y_sum_level1a, multistart = 4)
#toc(log = TRUE)

```


```{r}


#Ytest <- vector(mode = 'list', length = ncol(Y_sum_level1a))
#for(i in 1:2){
  
#  Ytest[[i]] <- Y_sum_level1a[, i]
#}



#tic()
#test <- lapply(X = Y_sum_level1a_list, FUN = km, formula = ~., design = X_level1a)
#toc(log = TRUE)

#tic()
#partest <- mclapply(X = Y_sum_level1a_list, FUN = km, formula = ~., design = X_level1a, mc.cores = 4) 
#toc(log = TRUE)


```







```{r, fig.width = 12, fig.height = 12}
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,4,0.1))
for(i in 1:length(loolist_km_Y)){
  
  y <- Y_level1[, y_names_sum[i]]
  loo <- loolist_km_Y[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = y_names_sum[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)
  legend('bottomright',legend = paste('mae =',round(loostats_km_Y[[i]]$pmae,2),'%') , bty = 'n')

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 
mtext('Modern Value (Y)', side = 3, line = 0, outer = TRUE, cex = 2) 

```


```{r, fig.width = 12, fig.height = 6, results = 'hide'}

require(foreach)
nCores <- 4
require(doParallel)
cl <-  makeCluster(nCores) 
registerDoParallel(cl)

if (file.exists("km_emulators_Ylevel1a.rdata")) {
  load("km_emulators_Ylevel1a.rdata")
} else {
  
  emlist_km_Ylevel1a <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    yname <- y_names_sum[i]
    y <- Y_level1a[, yname]
    
    em <- km(~., design = X_level1a, response = y, multistart = 4)
    emlist_km_Ylevel1a[[i]] <- em
  }
  
  loolist_km_Ylevel1a <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    
    loo <- leaveOneOut.km(model = emlist_km_Ylevel1a[[i]], type = 'UK', trend.reestim = TRUE)
    loolist_km_Ylevel1a[[i]] <- loo
  }
  
  save(emlist_km_Ylevel1a,loolist_km_Ylevel1a, file = "km_emulators_Ylevel1a.rdata")
}

stopCluster(cl)



```


```{r}


loostats_km_Ylevel1a <- vector(mode = 'list', length = length(y_names_sum))

for(i in 1:length(emlist_km_Ylevel1a)){
  
  loostats <- kmLooStats(emlist_km_Ylevel1a[[i]])
  loostats_km_Ylevel1a[[i]] <- loostats
  print(loostats$pmae)
}

```



```{r}
# level1 vs level 1a
km_pmae_level1 <- sapply(loostats_km_Y, function(x) x$pmae)
km_pmae_level1a <- sapply(loostats_km_Ylevel1a, function(x) x$pmae)

plot(km_pmae_level1 , km_pmae_level1a )
abline(0,1)

```









```{r, fig.width = 12, fig.height = 6, results = 'hide'}

if (file.exists("km_emulators_YAnom.rdata")) {
  load("km_emulators_YAnom.rdata")
} else {
  
  emlist_km_YAnom <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    yname <- y_names_sum[i]
    y <- YAnom_level1[, yname]
    
    em <- km(~., design = X_level1, response = y)
    emlist_km_YAnom[[i]] <- em
  }
  
  loolist_km_YAnom <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    
    loo <- leaveOneOut.km(model = emlist_km_YAnom[[i]], type = 'UK', trend.reestim = TRUE)
    loolist_km_YAnom[[i]] <- loo
  }
  
  save(emlist_km_YAnom,loolist_km_YAnom, file = "km_emulators_YAnom.rdata")
}

```


# km emulators for change in variables over time

```{r, fig.width = 12, fig.height = 12}
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,4,0.1))
for(i in 1:length(loolist_km_YAnom)){
  
  y <- YAnom_level1[, y_names_sum[i]]
  loo <- loolist_km_YAnom[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = y_names_sum[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)
  

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 
mtext('Change over time (YAnom)', side = 3, line = 1, outer = TRUE, cex = 2) 

```

Next, plot the twostep glmnet/km emulators

```{r, fig.width = 12, fig.height = 6}
# Twostep glmnet emulators

if (file.exists("ts_emulators_Y.rdata")) {
  load("ts_emulators_Y.rdata")
} else {
  
  emlist_twoStep_glmnet_Y <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    
    yname <- y_names_sum[i]
    y <- Y_level1[, yname]
    
    em <- twoStep_glmnet(X = X_level1, y = y)
    emlist_twoStep_glmnet_Y[[i]] <- em
  }
  
  loolist_twoStep_glmnet_Y <- vector(mode = 'list', length = length(y_names_sum))
  
  for(i in 1:length(y_names_sum)){
    
    loo <- leaveOneOut.km(model = emlist_twoStep_glmnet_Y[[i]]$emulator, type = 'UK', trend.reestim = TRUE)
    loolist_twoStep_glmnet_Y[[i]] <- loo
  }
  
  
  
  save(emlist_twoStep_glmnet_Y, loolist_twoStep_glmnet_Y, file = "ts_emulators_Y.rdata")

}


```

```{r}
# Can get numerical performance using

loostats_twoStep_glmnet_Y <- vector(mode = 'list', length = length(y_names_sum))

for(i in 1:length(emlist_twoStep_glmnet_Y)){
  
  loostats <- kmLooStats(emlist_twoStep_glmnet_Y[[i]]$emulator)
  loostats_twoStep_glmnet_Y[[i]] <- loostats
  print(loostats$pmae)
}


```



## TwoStep emulator performance for modern values

```{r, fig.width = 12, fig.height = 12}
par(mfrow = c(4,4), mar = c(3,4,2,2), oma = c(4,4,4,0.1))
for(i in 1:length(loolist_twoStep_glmnet_Y)){
  
  y <- Y_level1[, y_names_sum[i]]
  loo <- loolist_twoStep_glmnet_Y[[i]]
  ylim <- range(c(loo$mean - (2*loo$sd), loo$mean + (2*loo$sd)) )
  plot(y, loo$mean, xlab = '', ylab = '', main = y_names_sum[i] , ylim = ylim, col = makeTransparent('black', 70),
       pch = 19)
  segments(x0 = y, y0 = loo$mean - (2*loo$sd)  , x1 = y , y1 = loo$mean + (2*loo$sd), col = makeTransparent('black', 70))
  abline(0,1)
  legend('bottomright',legend = paste('mae =',round(loostats_twoStep_glmnet_Y[[i]]$pmae,2),'%') , bty = 'n')

}

mtext('Actual', side = 1, line = 1, outer = TRUE, cex = 2 )
mtext('Predicted', side = 2, line = 0, outer = TRUE, cex = 2) 
mtext('Modern Value (Y)', side = 3, line = 0, outer = TRUE, cex = 2) 

```

# Compare Leave-one-out statistics for the two types of emulator

We use the leave-one-out Mean Absolute Error, expressed as a percentage of the range of the output across the ensemble. We find that the twostep emulatorisn't significantly more accurate, and is indeed *less* accurate for tree fraction.

```{r, fig.width = 8, fig.height = 7}


km_pmae <- sapply(loostats_km_Y, '[[', 'pmae')
ts_pmae <- sapply(loostats_twoStep_glmnet_Y, '[[', 'pmae')

par(mar = c(12,4,2,1), las =1 )
plot(1:length(y_names_sum), km_pmae,
     ylim = c(0,15), pch = 19,
     axes = FALSE, xlab = '', 
     ylab = 'LOO MAE (% of range)',
     cex = 1.2)
points(1:length(y_names_sum),ts_pmae , col= 'red', pch = 19, cex = 1.2)
legend('topleft', c('km emulator', 'twoStep emulator'), pch = 19, col = c('black', 'red'), pt.cex = 1.2)
axis (2)
par(las = 2)
axis(1, at = 1:length(y_names_sum), labels = y_names_sum)

```



